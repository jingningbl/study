{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5]).filter(lambda x : x%2 == 0).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 7, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 6, 7, 8]).top(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 6, 7, 8]).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2', 5), ('d', 4), ('1', 3), ('b', 2), ('a', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
    "sc.parallelize(tmp).sortBy(lambda x: x[1],False).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 3]), ('b', [2])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([('a', 1), ('b', 2), ('a', 3),]).groupByKey().mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1, 2), (3, 4)]).keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(1, 2), (3, 4)]).values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1, 2, 3, 4, 5]).reduce(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([4, 6, 8, 2, 9]).takeOrdered(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([10, 4, 2, 12, 3]).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'a': 2, 'b': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)]).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
    "rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
    "rdd1.intersection(rdd2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what is your name', 17), ('my name is john', 15), ('nice to meet you', 16)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list01 = [\"what is your name\", \"my name is john\", \"nice to meet you\"]\n",
    "#（1）请通过RDD编程计算list01中各个元素的长度。并通过图示画出RDD的转换过程\n",
    "sc.parallelize(list01).map(lambda x:(x,len(x))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('what', 4),\n",
       " ('is', 2),\n",
       " ('your', 4),\n",
       " ('name', 4),\n",
       " ('my', 2),\n",
       " ('name', 4),\n",
       " ('is', 2),\n",
       " ('john', 4),\n",
       " ('nice', 4),\n",
       " ('to', 2),\n",
       " ('meet', 4),\n",
       " ('you', 3)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#（2）请通过RDD编程计算list01中各个元素中各个单词的长度。并通过图示画出RDD的\n",
    "sc.parallelize(list01).flatMap(lambda x:x.split(' ')).map(lambda x:(x,len(x))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------+\n",
      "| id|name|score|province|\n",
      "+---+----+-----+--------+\n",
      "|003|王五|   95|    河南|\n",
      "|002|李四|   91|    山西|\n",
      "+---+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"001\", \"张明\", 83, \"河南\"), (\"002\", \"李四\", 91, \"山西\"), (\"003\", \"王五\", 95, \"河南\"), (\"004\", \"赵柳\", 76, \"河北\")]\n",
    "#请通过DataFrame API和SparkSQL分别编程\n",
    "#（1）过滤出分数大于90的学生，并按分数由高到低显示\n",
    "#DataFrame\n",
    "data=spark.createDataFrame(data).withColumnRenamed(\"_1\",\"id\").withColumnRenamed(\"_2\",\"name\").withColumnRenamed(\"_3\",\"score\").withColumnRenamed(\"_4\",\"province\")\n",
    "data.filter(data.score>90).sort(-data[\"score\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-----+--------+\n",
      "| id|name|score|province|\n",
      "+---+----+-----+--------+\n",
      "|003|王五|   95|    河南|\n",
      "|002|李四|   91|    山西|\n",
      "+---+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#spark sql\n",
    "data.createTempView(\"view1\")\n",
    "spark.sql(\"select id,name,score,province from view1 where score > 90 order by score desc\").show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9cc273a5df71486cc71a39a261bf6d82d026507ff1b51fa9f8def4a2173a39cc"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
