{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fbb97e",
   "metadata": {},
   "source": [
    "## RDD & DataFrame编程步骤："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24697854",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| RDD编程步骤        | DataFrame编程步骤         |\n",
    "| -------------:|:-------------|\n",
    "| step 1: 创建SparkContext对象      |  step 1: 创建SparkSession对象 |\n",
    "| step 2: 通过SparkContext对象创建RDD     | step 2: 通过SparkSession对象创建DataFrame     |\n",
    "| step 3: 对RDD进行Transformation操作（预处理） | step 3: 对DataFrame进行Transformation操作（预处理）     |\n",
    "||## 通过DataFrame API 提供的方法|\n",
    "||## 通过Spark SQL|\n",
    "| step 4: 对RDD进行Action操作（输出结果）     | step 4: 对DataFrame进行Action操作（输出结果） |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65432f48",
   "metadata": {},
   "source": [
    "## RDD编程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df41bf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b4d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd01 = sc.parallelize([[1, 2], [3, 4], [5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db630f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd02 = sc.parallelize(('how are you', 'i am fine'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5b790f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd03 = sc.textFile('StreamingData01\\\\sentences01.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128b05aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12ccf682",
   "metadata": {},
   "source": [
    "## DataFrame编程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab485406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b82e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ((1001, \"北京\", 335), (1002, \"上海\", 495), (1003, \"郑州\", 418))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d1f2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df01 = spark.createDataFrame(data)                                                                                \n",
    "df01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfc83df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
